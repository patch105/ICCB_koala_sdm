---
title: "ICCB Species distribution modelling and validation"
author: "Scott Forrest and Charlotte Patterson"
date: "`r Sys.Date()`"
execute: 
  cache: false
# bibliography: references.bib
toc: true
number-sections: false
format: 
  html:
    self-contained: true
    code-fold: show
    code-tools: true
    df-print: paged
    code-line-numbers: true
    code-overflow: scroll
    fig-format: png
    fig-dpi: 300
  pdf:
    geometry: 
      - top=30mm
      - left=30mm
editor:
  source
  markdown: 
    wrap: 72
abstract: |
  In this script we are fitting species distribution models to the data of koalas (Phascolarctos cinereus) in the South-East Queensland (SEQ) region under current environmental conditions, which we will predict into future environmental conditions. 
---

## Import packages

```{r}

library(dplyr)
library(purrr)
library(ggplot2)
library(terra)
library(sf)
library(predicts)
library(blockCV)
library(ecospat)
library(usdm)
library(randomForest)
library(precrec)
library(corrplot)

```

## Load koala presences and background points

They are loaded as spatvectors, but we also want them as dataframes for model input requirements.

```{r}

koala_occ <- vect("Data/Biological_records/SEQ_koala_occurrences.shp")
background <- vect("Data/Biological_records/background_points_50k_random.shp")

# Make a dataframe of just x, y and presence
koala_occ_df <- koala_occ %>% 
  as.data.frame(geom = "XY") %>% 
  dplyr::select(x,y) %>% 
  mutate(Presence = 1)

head(koala_occ_df)

background_df <- background %>% 
  as.data.frame(geom = "XY") %>% 
  dplyr::select(x,y) %>% 
  mutate(Presence = 0)

head(background_df)

# Combine to one
pr_bg <- rbind(koala_occ_df, background_df)

```

## Load environmental covariates

Loading current covariate rasters.

```{r}

covs_current <- rast("Data/Environmental_variables/current_bioclim.tif")


# Define the BIOCLIM names for the raster layers
layer_names <- c(
  "BIO1_Annual_Mean_Temp",
  "BIO2_Mean_Diurnal_Temp_Range",
  "BIO3_Isothermality",
  "BIO4_Temperature_Seasonality",
  "BIO5_Max_Temp_Warmest_Month",
  "BIO6_Min_Temp_Coldest_Month",
  "BIO7_Temperature_Annual_Range",
  "BIO8_Mean_Temp_Wettest_Quarter",
  "BIO9_Mean_Temp_Driest_Quarter",
  "BIO10_Mean_Temp_Warmest_Quarter",
  "BIO11_Mean_Temp_Coldest_Quarter",
  "BIO12_Annual_Precipitation",
  "BIO13_Precip_Wettest_Month",
  "BIO14_Precip_Driest_Month",
  "BIO15_Precip_Seasonality",
  "BIO16_Precip_Wettest_Quarter",
  "BIO17_Precip_Driest_Quarter",
  "BIO18_Precip_Warmest_Quarter",
  "BIO19_Precip_Coldest_Quarter")

names(covs_current) <- layer_names

```

## Covariate selection

### Option 1. Narrow down potential covariates based on ecological knowledge

For this example, we had advice from CSIRO scientists who conducted an expert elicitation to gather a set of potential covariates that are likely to be important for koalas. We use this knowledge to filter out the key bioclim variables.

We select the following: Bio5 : Max temp of the warmest month (mainly for the northern populations) Bio6 : Min temp of the coldest month (mainly for southern populations, which essentially excludes alpine regions) Bio12 : Annual Precipitation Bio15 : Precipitation seasonality (coefficient of variation)

```{r}

for(i in 1:nlyr(covs_current)) {
  terra::plot(covs_current[[i]], main = names(covs_current)[[i]])
}

```

## Show the four from expert elicitation the layers

```{r}

covs_current_expert <- subset(covs_current, names(covs_current) %in% c("BIO5_Max_Temp_Warmest_Month", 
                                                                       "BIO6_Min_Temp_Coldest_Month", 
                                                                       "BIO12_Annual_Precipitation", 
                                                                       "BIO15_Precip_Seasonality"))

for(i in 1:nlyr(covs_current_expert)) {
  terra::plot(covs_current_expert[[i]], main = names(covs_current_expert)[[i]])
}

```

## Extract environmental covariate values from presence and background locations (training locations)

```{r}

train_PB_covs <- terra::extract(covs_current, pr_bg[,c("x", "y")], xy = T)
train_PB_covs <- cbind(train_PB_covs, pr_bg["Presence"])

# Remove rows where there's values missing from at least one covariate
print(paste0("RECORDS FROM ", nrow(train_PB_covs) - sum(complete.cases(train_PB_covs)), " ROWS IN TRAINING DATA REMOVED DUE TO MISSING COVARIATE VALUES"))

train_PB_covs <- train_PB_covs[complete.cases(train_PB_covs), ] 
train_PB_covs <- dplyr::select(train_PB_covs, -ID)

head(train_PB_covs)

```

## Check correlation and multicollinearity of covariates

### Correlation plot

There are several different methods for creating correlation plots. 

```{r}

ecospat.cor.plot(covs_current_expert)

```

### Using the corplots package

For a simple and quick plot.

```{r}

corplots <- ENMTools::raster.cor.plot(covs_current_expert)
corplots$cor.mds.plot
corplots$cor.heatmap

```

Here, we use the `corrplot` package to create a correlation plot of the selected covariates (this is taken from an EcoCommons Australia notebook).

```{r}

# Select columns by their names
cor_data <- train_PB_covs[, names(train_PB_covs) %in% c("BIO5_Max_Temp_Warmest_Month", 
                                                        "BIO6_Min_Temp_Coldest_Month", 
                                                        "BIO12_Annual_Precipitation", 
                                                        "BIO15_Precip_Seasonality")]

# Check the structure of the numeric data
str(cor_data)

# Calculate the correlation matrix for the numeric columns
cor_matrix <- cor(cor_data, use = "complete.obs", method = "pearson")


corrplot(cor_matrix,
         method = "color",            # Use colored squares for correlation
         type = "upper",              # Show upper triangle only
         order = "hclust",            # Reorder variables hierarchically
         addCoef.col = "black",       # Show correlation coefficients in black
         number.cex = 0.5,            # Reduce the size of correlation labels
         tl.col = "black",            # Text label color
         tl.srt = 30,                 # Rotate labels slightly for readability
         tl.cex = 0.5,                # Reduce text size of variable labels (set smaller valu)
         cl.cex = 0.8,                # Reduce text size of color legend
         diag = FALSE,                # Hide diagonal
         col = colorRampPalette(c("#11aa96", "#61c6fa", "#f6aa70"))(200),
         sig.level = 0.01, insig = "blank")

```

### Variance Inflation Factor (VIF)

If you find corrplot is hard for you to make decisions, we can use Variance Inflation Factor (VIF). VIF is another statistical measure used to detect multicollinearity in a set of explanatory (independent) variables in a regression model.

**Interpretation:**

-   VIF = 1: No correlation
-   VIF \> 1 and \<= 5: Moderate correlation; may not require corrective action.
-   VIF \> 5: Indicates high correlation. Multicollinearity may be problematic, and further investigation is recommended.
-   VIF \> 10: Strong multicollinearity. The variable is highly collinear with others, and steps should be taken to address this.

```{r}

# usdm::vif(covs_current_expert) # just VIF for all covariates
usdm::vifstep(covs_current_expert) # Variance Inflation Factor and test for multicollinearity

```

## Exploration of the data

It is good practice to assess where in the environmental space the presence and background points are located. This can help to identify if there are any potential issues with the data, such as a lack of background points in certain areas of environmental space, and should show any patterns in the data that the model should pick up.

```{r}

train_PB_covs_pres <- train_PB_covs %>% filter(Presence == 1)
train_PB_covs_bg <- train_PB_covs %>% filter(Presence == 0)

# Thin the presences for plotting
train_PB_covs_pres_thin <- train_PB_covs_pres[sample(nrow(train_PB_covs_pres), 10000), ]

# Combine back into both presence and background
train_PB_covs_thinned <- rbind(train_PB_covs_pres_thin, train_PB_covs_bg)

# Iterate over all of the variables to create density plots of the background and presence data
for(i in 1:ncol(train_PB_covs_thinned)) {
  
  print(ggplot() +
          geom_density(data = train_PB_covs_thinned, 
                       aes(x = .data[[names(train_PB_covs_thinned)[i]]], fill = as.factor(Presence)), 
                       alpha = 0.5) +
    theme_bw() +
    labs(title = names(train_PB_covs_thinned)[i]))
  
}

```

# GLM model fitting

```{r}

# Make a folder to save outputs
dir.create("Outputs/GLM_outputs", showWarnings = F)

```

## Null model

Null model: no explanatory variables or predictors are included.

It is always helpful to create a null model as a benchmark to assess how the inclusion of explanatory variables improves the model.

```{r}

# Fit a null model with only the intercept
null_model <- glm(Presence ~ 1,
                  data = train_PB_covs,
                  family = binomial(link = "logit"))

# Check the model results
summary(null_model)

```

## GLM Model 1 - expert variables

```{r}

glm_model_1 <- glm(Presence ~ 
                     BIO5_Max_Temp_Warmest_Month + 
                     BIO6_Min_Temp_Coldest_Month + 
                     BIO12_Annual_Precipitation + 
                     BIO15_Precip_Seasonality,
                   data=train_PB_covs_thinned,
                   family = binomial(link = "logit"))

# Check the model results
summary(glm_model_1)

dismo::response(glm_model_1)

```

## GLM Model 2 - expert variables with quadratics

```{r}

glm_model_2 <- glm(Presence ~ 
                     BIO5_Max_Temp_Warmest_Month + I(BIO5_Max_Temp_Warmest_Month^2) + 
                     BIO6_Min_Temp_Coldest_Month + I(BIO6_Min_Temp_Coldest_Month^2) + 
                     BIO12_Annual_Precipitation + I(BIO12_Annual_Precipitation^2) + 
                     BIO15_Precip_Seasonality + I(BIO15_Precip_Seasonality^2), 
                   data=train_PB_covs_thinned,
                   family = binomial(link = "logit"))

# Check the model results
summary(glm_model_2)

dismo::response(glm_model_2)

```

## Model evaluation

Here we use a function presented in an EcoCommons Australia notebook to evaluate the model performance. The notebook can be found on their GitHub: https://github.com/EcoCommonsAustralia/notebooks/tree/main/notebooks.

```{r}

# Function to plot effect size graph
plot_effect_size <- function(glm_model) {
  # Check if required libraries are installed
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Please install the 'ggplot2' package to use this function.")
  }
  library(ggplot2)

  # Extract effect sizes (coefficients) from the model
  coefs <- summary(glm_model)$coefficients
  effect_sizes <- data.frame(
    Variable = rownames(coefs)[-1],  # Exclude the intercept
    Effect_Size = coefs[-1, "Estimate"],
    Std_Error = coefs[-1, "Std. Error"]
  )

  # Sort by effect size
  effect_sizes <- effect_sizes[order(-abs(effect_sizes$Effect_Size)), ]

  # Plot the effect sizes with error bars
  ggplot(effect_sizes, aes(x = reorder(Variable, Effect_Size), y = Effect_Size)) +
    geom_bar(stat = "identity", fill = "#11aa96") +
    geom_errorbar(aes(ymin = Effect_Size - Std_Error, ymax = Effect_Size + Std_Error), width = 0.2) +
    coord_flip() +
    labs(
      title = "Effect Sizes of Variables",
      x = "Variable",
      y = "Effect Size (Coefficient Estimate)"
    ) +
    theme_minimal()
}

```

## Use the function to check the effect sizes

We need to be careful when interpreting the effect sizes of models with quadratic terms however, as the response curve depends on the linear and the quadratic term.

```{r}  

# Example usage of effect size plot
plot_effect_size(glm_model_1)
plot_effect_size(glm_model_2)

```

## Response curves

Again, we can use a function from the EcoCommons notebook to plot the response curves from the model, although for quadratics we need to adjust the function or use something else.

```{r}

plot_species_response <- function(glm_model, predictors, data) {
  # Check if required libraries are installed
  if (!requireNamespace("ggplot2", quietly = TRUE) || !requireNamespace("gridExtra", quietly = TRUE)) {
    stop("Please install the 'ggplot2' and 'gridExtra' packages to use this function.")
  }
  library(ggplot2)
  library(gridExtra)

  # Create empty list to store response plots
  response_plots <- list()

  # Loop through each predictor variable
  for (predictor in predictors) {
    # Create new data frame to vary predictor while keeping others constant
    pred_range <- seq(
      min(data[[predictor]], na.rm = TRUE),
      max(data[[predictor]], na.rm = TRUE),
      length.out = 100
    )
    const_data <- data[1, , drop = FALSE]  # Use first row to keep other predictors constant
    response_data <- const_data[rep(1, 100), ]  # Duplicate the row
    response_data[[predictor]] <- pred_range

    # Predict probabilities
    predicted_response <- predict(glm_model, newdata = response_data, type = "response")

    # Create data frame for plotting
    plot_data <- data.frame(
      Predictor_Value = pred_range,
      Predicted_Probability = predicted_response
    )

    # Add presence and absence data
    presence_absence_data <- data.frame(
      Predictor_Value = data[[predictor]],
      Presence_Absence = data$Presence
    )

    # Generate the response plot
    p <- ggplot() +
      
      geom_line(data = plot_data, 
                aes(x = Predictor_Value, y = Predicted_Probability), 
                color = "#61c6fa", linewidth = 1) +
      
      geom_point(data = presence_absence_data[presence_absence_data$Presence_Absence == 1, ], 
                 aes(x = Predictor_Value, y = Presence_Absence), 
                 color = "#11aa96", alpha = 0.6) +
      
      geom_point(data = presence_absence_data[presence_absence_data$Presence_Absence == 0, ], 
                 aes(x = Predictor_Value, y = Presence_Absence), 
                 color = "#f6aa70", alpha = 0.6) +
      
      labs(x = predictor, y = NULL) +
      theme_minimal() +
      theme(axis.title.y = element_blank())

    # Store the plot in the list
    response_plots[[predictor]] <- p
  }

  # Arrange all plots in one combined plot with a single shared y-axis label
  grid.arrange(
    grobs = response_plots,
    ncol = 3,
    left = "Predicted Probability / Presence-Absence"
  )
}

```

## Use the response curve function

```{r}

predictors <- c("BIO5_Max_Temp_Warmest_Month", "BIO6_Min_Temp_Coldest_Month", "BIO12_Annual_Precipitation", "BIO15_Precip_Seasonality")
plot_species_response(glm_model_1, predictors, train_PB_covs)

```

### Extract the coefficients for the linear and quadratic terms

```{r}

bio5_linear <- glm_model_2$coefficients["BIO5_Max_Temp_Warmest_Month"]
bio5_quadratic <- glm_model_2$coefficients["I(BIO5_Max_Temp_Warmest_Month^2)"]

bio6_linear <- glm_model_2$coefficients["BIO6_Min_Temp_Coldest_Month"]
bio6_quadratic <- glm_model_2$coefficients["I(BIO6_Min_Temp_Coldest_Month^2)"]

bio12_linear <- glm_model_2$coefficients["BIO12_Annual_Precipitation"]
bio12_quadratic <- glm_model_2$coefficients["I(BIO12_Annual_Precipitation^2)"]

bio15_linear <- glm_model_2$coefficients["BIO15_Precip_Seasonality"]
bio15_quadratic <- glm_model_2$coefficients["I(BIO15_Precip_Seasonality^2)"]

```

### Create some resonse curves

```{r}

# Create a sequence of values for the predictor variable
bio5_seq <- seq(min(train_PB_covs$BIO5_Max_Temp_Warmest_Month, na.rm = TRUE), 
                 max(train_PB_covs$BIO5_Max_Temp_Warmest_Month, na.rm = TRUE), 
                 length.out = 100)

# Calculate the predicted probabilities for the linear and quadratic terms
bio5_pred_linear <- bio5_linear * bio5_seq
bio5_pred_quadratic <- bio5_quadratic * (bio5_seq^2)
# Now sum together to get the BIO5 response
bio5_pred <- bio5_pred_linear + bio5_pred_quadratic

# Combine the results into a data frame for plotting
bio5_plot_data <- data.frame(
  BIO5_Max_Temp_Warmest_Month = bio5_seq,
  Predicted_Probability = bio5_pred
)

ggplot() +
  geom_line(data = bio5_plot_data, aes(x = BIO5_Max_Temp_Warmest_Month, y = Predicted_Probability), color = "#61c6fa", linewidth = 1) +
  labs(x = "BIO5: Max Temp of Warmest Month", y = "Predicted Probability") +
  theme_minimal() +
  theme(axis.title.y = element_blank())

```


## GLM predictions

### Model 1

```{r}

# Predict the presence probability across the entire raster extent
predicted_raster_model_1 <- predict(covs_current_expert, glm_model_1, type = "response")

# Plot the species distribution raster
plot(
  predicted_raster_model_1,
  main = "Predicted Probability of Presence of Koalas in SEQ"
)

```

### Model 2

```{r}

# Predict the presence probability across the entire raster extent
predicted_raster_model_2 <- predict(covs_current_expert, glm_model_2, type = "response")

# Plot the species distribution raster
plot(
  predicted_raster_model_2,
  main = "Predicted Probability of Presence of Koalas in SEQ"
)

```






## Random forest

```{r}

# Make a folder to save outputs

dir.create("Outputs/RF_outputs", showWarnings = F)

```

Normalise the covariates

```{r}

for (v in cov_names) {
  
   meanv <- mean(train_PB_covs[, v])
   sdv <- sd(train_PB_covs[, v])
   train_PB_covs_scv_norm[, v] <- (train_PB_covs[, v] - meanv) / sdv
   covs_current_norm[, v] <- (covs_current[, v] - meanv) / sdv

  }

```

Calculate the case weights (down-weighting)

```{r}

prNum <- as.numeric(table(train_PB_covs$Presence)["1"]) # number of presences
bgNum <- as.numeric(table(train_PB_covs$Presence)["0"]) # number of backgrounds
wt <- ifelse(train_PB_covs$Presence == 1, 1, prNum / bgNum) # down-weighting
  

```



```{r}

# Convert the response to factor for producing class relative likelihood
  train_PB_covs$Presence <- as.factor(train_PB_covs$Presence)
  
  # For down-sampling, the number of background (0s) in each bootstrap sample should the same as presences
  # (1s). For this, we use sampsize argument to do this.
  smpsize <- c("0" = prNum, "1" = prNum)
  
  rf <- randomForest::randomForest(formula = Presence ~.,
                                   data = train_PB_covs,
                                   ntree = 1000,
                                   sampsize = smpsize,
                                   replace = T)

```

## Looking at model results

-   SCOTT - CHOOSE SOME STUFF TO LOOK AT E.G. PARTIAL EFFECTS, COEFFICIENTS ETC.

## Predict to the current environmental conditions

```{r}

# Current prediction
pred_current_maxent <- dismo::predict(maxent.mod, covs_current, args = "doclamp=false")
pred_current_maxent <- cbind(covs_current, pred_current_maxent)
colnames(pred_current_maxent)[grepl("pred", colnames(pred_current_maxent))] <- "pred"

pred_cur.rast <- rast(pred_current_maxent[, c("x", "y", "pred")], type = "xyz", crs = "EPSG:3112")

plot(pred_cur.rast)

```

## Sampling bias

-   SCOTT - GET ACCESSIBILITY LAYER READY (MAYBE IN ENVIRO_DATA FOLDER) AND ADD TO MODEL HERE

Now, we will re-run the above model with a covariate to account for sampling bias. We'll plot the models with and without a sampling covariate to see the difference in models.

```{r}



```

## Model evaluation with spatial block cross-validation

First, we're going to randomly subset the data because this is just an indicative example and it will make things run faster. \*SCOTT - EITHER SUBSET DATA HERE OR SAVE THE SPATIAL FOLDS BELOW SO THAT PEOPLE CAN JUST LOAD THEM RATHER THAN RUNNING CV_SPATIAL FUNCTION

```{r}



```



```{r}

# Convert training data to sf
train_PB_sf <- st_as_sf(train_PB_covs[, c("x", "y", "Presence")], coords = c("x", "y"), crs = "EPSG:3112")

# Generate spatial blocks
spblock <- cv_spatial(x = train_PB_sf, 
                      column = "Presence",
                      r = NULL,
                      size = 50000, # Size of the blocks in metres
                      k = 5,
                      hexagon = TRUE,
                      selection = "random",
                      iteration = 100, # to find evenly-dispersed folds
                      biomod2 = FALSE)

cv_plot(cv = spblock,
        x = train_PB_sf,
        points_alpha = 0.5,
        nrow = 2)


# Extract the folds to save 
spfolds <- spblock$folds_list

# We now have a list of 5 folds, where the first object is the training data, and the second is the testing data
str(spfolds)

```

# Run the model for every fold and evaluate

## Model evaluation - metrics

Typically, it helps to evaluate your model with several metrics that describe different features of model performance and prediction. Here, we define a function to feed in a model prediction and calculate several evaluation metrics.

The metrics are: -Area under the receiver operating characteristic curve (AUC ROC) -Continuous boyce index

```{r}

  # Make an evaluation function
  evaluate_prediction <- function(x){
  
  ROC = precrec::auc(precrec::evalmod(scores = x$pred, labels = x$Presence))[1,4]
 
  boyce = ecospat::ecospat.boyce(fit = x$pred, 
                                 obs = x$pred[which(x$Presence==1)], 
                                 nclass = 0, # Calculate continuous index
                                 method = "pearson",
                                 PEplot = T)[["cor"]]
  
  eval_df <- data.frame(ROC = ROC,
                        boyce = boyce)
                        
 return(eval_df)
  
}

```

```{r}

# Start a dataframe to save results
eval_df <- data.frame(fold = numeric(),
                      ROC = numeric(),
                      boyce = numeric())

for(f in seq_along(spfolds)) {
  
  # Subset the training and testing data (spatial cross validation) (for the fth fold)
  
  train_PB_covs_scv <- train_PB_covs[spfolds[[f]][[1]], ]
  test_PB_covs_scv <- train_PB_covs[spfolds[[f]][[2]], ]
  
  maxent.mod <- NULL
  maxent.mod <- dismo::maxent(x = select(train_PB_covs_scv, -c(Presence, x, y)),
                              p = train_PB_covs_scv[["Presence"]],
                              removeDuplicates = FALSE,
                              path = "Outputs/Maxent_outputs",
                              args = c("nothreshold"))
  
  # Predict to the testing data of fold f
  pred_test.mxt <- dismo::predict(maxent.mod, test_PB_covs_scv, args = "doclamp=false")
  pred_test.mxt <- cbind(test_PB_covs_scv, pred_test.mxt)
  colnames(pred_test.mxt)[grepl("pred", colnames(pred_test.mxt))] <- "pred"
  
  # Evaluate prediction on test set
  ROC = precrec::auc(precrec::evalmod(scores = pred_test.mxt$pred, labels = pred_test.mxt$Presence))[1,4]
 
  boyce = ecospat::ecospat.boyce(fit = pred_test.mxt$pred, 
                                 obs = pred_test.mxt$pred[which(pred_test.mxt$Presence==1)], 
                                 nclass = 0, # Calculate continuous index
                                 method = "pearson",
                                 PEplot = T)[["cor"]]
  
  # Add results to dataframe
  eval_df <- eval_df %>% add_row(fold = f, ROC = ROC, boyce = boyce)

  
}

```

## Summarise the evaluation metrics

```{r}

# Mean AUC & boyce
eval_df %>% 
  summarise(mean_AUC = mean(ROC),
            mean_boyce = mean(boyce))


```

## Load future environmental data

```{r}

covs_future <- rast("Data/Environmental_variables/future_bioclim.2090.SSP370.tif")
names(covs_future) <- layer_names
covs_future

covs_future_expert <- subset(covs_future, names(covs_future) %in% c("BIO5_Max_Temp_Warmest_Month", 
                                                                    "BIO6_Min_Temp_Coldest_Month", 
                                                                    "BIO12_Annual_Precipitation", 
                                                                    "BIO15_Precip_Seasonality"))

```

## GLM future predictions

### Model 1

```{r}

# Predict the presence probability across the entire raster extent
predicted_raster_model_1 <- predict(covs_future_expert, glm_model_1, type = "response")

# Plot the species distribution raster
plot(
  predicted_raster_model_1,
  main = "Predicted Probability of Presence of Koalas in SEQ"
)

```

```{r}

plot(covs_current_expert)
plot(covs_future_expert)

```


### Model 2

```{r}

# Predict the presence probability across the entire raster extent
predicted_raster_model_2 <- predict(covs_future_expert, glm_model_2, type = "response")

# Plot the species distribution raster
plot(
  predicted_raster_model_2,
  main = "Predicted Probability of Presence of Koalas in SEQ"
)

```

## Define environmental covariate values for prediction under *future* conditions

```{r}

pred_fut_covs <- as.data.frame(covs, xy = T)

print(paste0("RECORDS FROM ", nrow(pred_fut_covs) - sum(complete.cases(pred_fut_covs)), " ROWS IN PREDICTION DATA REMOVED DUE TO MISSING COVARIATE VALUES"))

# Remove rows with NA in any covariates
pred_fut_covs <- pred_fut_covs[complete.cases(pred_fut_covs), ]
pred_fut_covs <- dplyr::select(pred_fut_covs, -ID)

```

## Test the environmental distance between current data and future conditions

```{r}



```

## Sampling bias

```{r}

ggplot() +
  geom_sf(data = SEQ_extent, fill = "purple3", alpha = 0.5, color = "black", size = 0.2) +
  geom_sf(data = koala_occ_sf,                           # Add koala presence locations
          aes(geometry = geometry),
             color = "blue", size = 0.5) +               # Add points for occurrences
  ggtitle("Koala occurrences in South East Queensland") +      # Add title
  theme_bw()

```

# Human population density

https://qldspatial.information.qld.gov.au/catalogue/custom/detail.page?fid={36DBF62A-76E4-4BFA-A04A-E747401C4C09}

Some relevant details from the Metadata for this dataset:

POPULATION - The population (number of persons) indicated by the ABS Census figures that exist for this locality. This field will no longer be updated as of the 01/03/2016 due to changes in the source data from ABS data.

```{r}

pop.centre <- st_read("Data/Environmental_variables/Population_centres.shp")
pop.centre <- st_transform(pop.centre, crs = st_crs(SEQ_extent)) # Transform to match SEQ_extent

head(pop.centre)

# Trim to South East Queensland
pop.centre <- vect(pop.centre)

pop.centre <- mask(pop.centre, SEQ_extent.vect) %>% 
  filter(population != 0)

ggplot() +
  geom_sf(data = SEQ_extent, color = "black", size = 0.2) +
  geom_sf(data = pop.centre, color = "purple3", aes(size = population), alpha = 0.5) +
  theme_minimal() +
  labs(title = "Population density in cities and towns in SEQ")

```

```{r}

ggplot() +
  geom_sf(data = SEQ_extent, color = "black", size = 0.2) +
  geom_sf(data = koala_occ_sf,                           # Add koala presence locations
          aes(geometry = geometry),
             color = "blue", size = 0.5, alpha = 0.1) + 
  geom_sf(data = pop.centre, color = "purple3", aes(size = population)) +
  theme_minimal() +
  labs(title = "Population density in cities and towns in SEQ")


```

# Calculate distance to main centres

```{r}

dist_centres <- terra::distance(ext.rast, pop.centre)
dist_centres <- mask(dist_centres, ext.rast, maskvalue = NA)

ggplot() +
  geom_spatraster(data = dist_centres) +
  scale_fill_viridis_c() +
   theme_minimal() +
  labs(title = "Distance to cities and towns SEQ")

```

For city accessibility you can get a publicly available version using the following code:

```{r}

url <- 'https://figshare.com/ndownloader/files/14189843'

file <- 'Data/Environmental_variables/accessibility.tif'  # assuming you are in an R project folder that contains a “data” subfolder

options(timeout = 1500) # increased max downloading time to 25 min

download.file(url, file)

accessibility_rast <- rast(file)

NAflag(accessibility_rast) <- 65535 # reclass 65535 (water) as NA

subsection <- ext(112, 155, -45, -10) # extent to crop to include Australia only [here you can use your own area]

accessibility_rast <- crop(accessibility_rast, SEQ_extent)

# transform it by using square root to get a better model fit

accessibility_rast <- sqrt(accessibility_rast)

```

The human population density was taken from: https://www.abs.gov.au/AUSSTATS/abs\@.nsf/DetailsPage/1270.0.55.0072011?OpenDocument
